# CV_animal_video_detection
Overview of the methods used to detect animals in heterogeneous environments on video data 

# Исследование современных подходов к детекции животных на видео

Чтобы разобраться с задачей детекции животных на видеоданных и получения понимания о существующих подходах и методах в этой области я провел обзор публикаций, готовых проектов и доступных датасетов. Я не являюсь специалистом в компьютерном зрении, поэтому я постарался изучить уже существующие решения.  
Ссылки (материалы) разделены на три категории — публикации (исследования), готовые проекты и датасеты. Естественно это всего лишь беглый осмотр темы и "низковисящие яблоки".

## Research

Некоторые публикации включают анализ подходов глубокого обучения для детекции животных, изучение видеодатасетов, а также методы многообъектного отслеживания:

- [A Comprehensive Review of Deep Learning Approaches for Animal Detection on Video Data](https://www.researchgate.net/publication/376179857_A_Comprehensive_Review_of_Deep_Learning_Approaches_for_Animal_Detection_on_Video_Data)
- [A Video Dataset of Cattle Visual Behaviors](https://arxiv.org/pdf/2305.16555v1)
- [Multi-Object Tracking in Heterogeneous environments (MOTHe) for animal video recordings | PeerJ](https://peerj.com/articles/15573/)
- [MOTHe: Multi-Object Tracking in Heterogeneous environments - bioRxiv](https://www.biorxiv.org/content/10.1101/2020.01.10.899989v2.full)
- [MOTHe GUI на GitHub](https://github.com/tee-lab/MOTHe-GUI)

Эти исследования охватывают как теоретические аспекты использования глубоких нейронных сетей в задачах детекции животных, так и практические примеры с конкретными видеодатасетами. В частности, статья по **MOTHe** предлагает эффективные методы для отслеживания животных на сложных фонах в условиях естественной среды.

## Projects

Для практического применения в задачах детекции животных были найдены несколько проектов, реализующих модели на основе нейронных сетей. Эти проекты предлагают готовые решения для распознавания различных животных с помощью TensorFlow и других инструментов.

- [Animal Detection Using Tensorflow Object Detection API](https://medium.com/@mdtausifc/animal-detection-using-tensorflow-object-detection-api-859d6bd368dc) — статья с Medium
- [CMDTausif/Animal-Detection-Using-Tensorflow-Object-Detection-API на GitHub](https://github.com/CMDTausif/Animal-Detection-Using-Tensorflow-Object-Detection-API?source=post_page-----859d6bd368dc--------------------------------)
- [Animal detection using neural network in TensorFlow | Updates-Matter](https://stackforgeeks.com/blog/animal-detection-using-neural-network-in-tensorflow)
- [burnpiro/farm-animal-tracking: Farm Animal Tracking (FAT)](https://github.com/burnpiro/farm-animal-tracking)
- [iNaturalist - датасеты животных](https://github.com/inaturalist)

Эти проекты ориентированы на детекцию животных с использованием библиотек TensorFlow и OpenCV. Реализованные подходы позволяют эффективно обрабатывать видеопотоки и обучать модели для различных задач распознавания.

## Datasets

- [Video dataset of sheep activity for animal behavioral analysis via deep learning - PubMed](https://pubmed.ncbi.nlm.nih.gov/38328501/)
- [Video dataset of sheep activity for animal behavioral analysis via deep learning - ScienceDirect](https://www.sciencedirect.com/science/article/pii/S2352340924000015?via%3Dihub)
- [Video Dataset of Sheep Activity (Grazing, Running, Sitting) - Mendeley Data](https://data.mendeley.com/datasets/h5ppwx6fn4/1)
- [Video Dataset of Sheep Activity (Standing and Walking) - Mendeley Data](https://data.mendeley.com/datasets/w65pvb84dg/1)
- [CVB: A Video Dataset of Cattle Visual Behaviors | DeepAI](https://deepai.org/publication/cvb-a-video-dataset-of-cattle-visual-behaviors)
- [dtnguyen0304/sawit: A Small-Sized Animal Wild Image Dataset with Annotations](https://github.com/dtnguyen0304/sawit?tab=readme-ov-file)
- [Search | Kaggle](https://www.kaggle.com/search?q=animal+video+dataset+in%3Adatasets)
- [Version](https://universe.roboflow.com/ai-bd/animals_video2/dataset/1)
- [Machine Learning Datasets | Papers With Code](https://paperswithcode.com/datasets?q=animals&v=lst&o=match&mod=videos&page=1)
- [DeformingThings4D Dataset | Papers With Code](https://paperswithcode.com/dataset/deformingthings4d)
- [TGIF Dataset | Papers With Code](https://paperswithcode.com/dataset/tgif)
- [Animal Kingdom Dataset | Papers With Code](https://paperswithcode.com/dataset/animal-kingdom)
- [Camouflaged Animal Dataset Dataset | Papers With Code](https://paperswithcode.com/dataset/camouflaged-animal-dataset)
- [Causal Motion Segmentation in Moving Camera Videos](http://vis-www.cs.umass.edu/motionSegmentation/)
- [Lindenthal Camera Traps Dataset | Papers With Code](https://paperswithcode.com/dataset/lindenthal-camera-traps)
- [Lindenthal Camera Traps - LILA BC](https://lila.science/datasets/lindenthal-camera-traps/)
- [MGif Dataset | Papers With Code](https://paperswithcode.com/dataset/mgif)
- [TGIF-QA Dataset | Papers With Code](https://paperswithcode.com/dataset/tgif-qa)
- [Dynamic Replica Dataset | Papers With Code](https://paperswithcode.com/dataset/dynamic-replica)
- [3D-POP Dataset | Papers With Code](https://paperswithcode.com/dataset/3d-pop)
- [DogCentric Activity Dataset | Papers With Code](https://paperswithcode.com/dataset/dogcentric-activity)
- [SLT-Net: Implicit Motion Handling for Video Camouflaged Object Detection](https://xueliancheng.github.io/SLT-Net-project/)
- [MoCA-Mask Dataset | Papers With Code](https://paperswithcode.com/dataset/moca-mask)
- [Desert Lion Conservation Camera Traps - LILA BC](https://lila.science/datasets/desert-lion-conservation-camera-traps/)
- [iWildCam 2022 - LILA BC](https://lila.science/datasets/iwildcam-2022/)
- [Data Sets - LILA BC](https://lila.science/datasets)
- [Amur Tiger Re-identification - LILA BC](https://lila.science/datasets/atrw)
- [CaltechCameraTraps | Caltech Camera Trap dataset page](https://beerys.github.io/CaltechCameraTraps/)
- [visipedia/inat_comp: iNaturalist competition details](https://github.com/visipedia/inat_comp?tab=readme-ov-file)
- [visipedia/inat_comp: iNaturalist competition details](https://github.com/visipedia/inat_comp/tree/master)
- [Snapshot Serengeti - LILA BC](https://lila.science/datasets/snapshot-serengeti)


На этих платформах собраны видеодатасеты, предназначенные для анализа поведения животных и детекции в природной среде. Они включают активность различных животных (овцы, крупный рогатый скот, дикие животные, насекомые, рыбы) и обеспечивают необходимую разнообразность фона, условий освещения и типов поведения.

# Решение задачи детекции зайцев на видеопотоке: вопросы и ответы

## Задача
Система должна детектировать определенные объекты (зайцев) в видеопотоке, фиксируя их в условиях естественной среды обитания — леса и поля. Основные сложности включают переменное освещение, погодные условия и разнообразие растительности.

---

## Вопрос 1: Выбор алгоритмов детекции
**Оптимальные алгоритмы: современные методы на базе глубокого обучения** — способны справляться с изменяющимися условиями освещения и фона.

### Рекомендуемые алгоритмы:
- **YOLOv5 или SSD**: высокая скорость обработки, подходят для реального времени. Подход: однопроходная детекция объектов ([A Comprehensive Review of Deep Learning Approaches for Animal Detection on Video Data](https://www.researchgate.net/publication/376179857_A_Comprehensive_Review_of_Deep_Learning_Approaches_for_Animal_Detection_on_Video_Data)).

**Недостатки:** YOLO и SSD могут терять точность при детекции небольших объектов на дальнем расстоянии или при плотной растительности, что стоит учитывать при настройке модели.

- **Faster R-CNN и Mask R-CNN**: более высокая точность, возможность сегментации контуров зайцев. Подходят для сложных условий ([MOTHe GUI](https://github.com/tee-lab/MOTHe-GUI)).  

**Недостатки:** Сегментационные сети медленнее и требовательны к вычислительным ресурсам, поэтому их лучше использовать, если скорость не критична или в случаях, когда возможна предобработка кадров перед подачей в систему реального времени.

- **EfficientDet**: эффективен для детекции, экономит вычислительные ресурсы, но требует GPU для реального времени.

### Возможный общий подход к решению:

- Начать с использования YOLOv5 или EfficientDet для детекции объектов в кадре.
- Для устойчивого отслеживания объектов можно дополнить подход Kalman-фильтром или алгоритмом Хангариана для предсказания положения в следующем кадре, как реализовано в MOTHe. Это обеспечит непрерывность отслеживания объектов между кадрами и устранит «дрожание» детекции.
- При необходимости сегментации, если частично видимые объекты или контрастные формы животных создают ложные срабатывания, использовать Mask R-CNN.

### Дополнительные техники:

- Аугментация данных: Добавить вариации (яркость, контраст, шум) к тренировочному набору, чтобы обученная модель могла справляться с изменяющимся освещением и погодными условиями.
- Активное обучение: Постепенно добавлять в обучающий набор сложные для модели кадры, чтобы повысить ее устойчивость к нестандартным условиям.

---

## Вопрос 2: Архитектура модели
Для устойчивой детекции на фоне сложных условий предпочтительны архитектуры, сочетающие пространственные  (формы и контуры объектов) и временные (движение объектов) признаки.

### Рекомендованные архитектуры:
- **YOLOv5 и YOLOv7**: слои свертки, residual-блоки, объединяющий слой для детекции объектов.  
    - Особенности архитектуры: YOLO использует слои свёртки для выделения иерархических признаков и выходной слой, позволяющий делать предсказания по классам и границам объектов в одном проходе. Этот подход делает YOLO быстрым и оптимальным для задач реального времени.
    - Оптимальные слои: Сверточные слои с разными размерами рецептивных полей, residual-блоки для улучшения обучения, а также final output layer, объединяющий детекцию объектов в одном кадре.
- **EfficientDet с BiFPN**: с объединением признаков на разных уровнях для работы с объектами разного размера.
    - Особенности архитектуры: EfficientDet сочетает архитектуры EfficientNet для экстракции признаков с BiFPN (Bidirectional Feature Pyramid Network), что делает его экономичным с точки зрения вычислительных ресурсов и устойчивым к изменяющимся условиям.
    - Оптимальные слои: BiFPN для эффективного объединения признаков на разных уровнях (для работы с объектами разного размера и на разной глубине), scaling layers для адаптации модели под задачи с разными разрешениями изображений.
- **Mask R-CNN**: слой RPN для выделения областей интереса, ROIAlign для точной сегментации.
    - Особенности архитектуры: Mask R-CNN расширяет Faster R-CNN, добавляя слой для сегментации объектов, что позволяет точно выделить границы объектов. Это полезно, если требуется точная идентификация границ зайцев среди растительности.
    - Оптимальные слои: Region Proposal Network (RPN) для нахождения потенциальных регионов объектов, ROIAlign для более точного извлечения признаков из этих регионов и upsampling layers для улучшения разрешения сегментирующей маски.
- **3D-CNN (3D Convolutional Neural Networks)**: позволяют учитывать временные зависимости для более точного распознавания движения.
    - Особенности архитектуры: использует трехмерные сверточные слои, которые одновременно работают с пространственными и временными данными, что позволяет учитывать движение объектов между кадрами. Подходит для задач отслеживания, где важны как форма, так и движение.
    - Оптимальные слои: 3D сверточные слои и 3D max-pooling слои для объединения временных данных в видеопотоке.
- **ConvLSTM (Convolutional LSTM)**:
    - Особенности архитектуры: ConvLSTM объединяет CNN с Long Short-Term Memory (LSTM), что позволяет захватывать временные зависимости. LSTM помогает запоминать движение и изменения, устойчивы к фоновому шуму, переменам освещения и погодным условиям.
    - Оптимальные слои: Convolutional LSTM для обработки пространственно-временных признаков, обеспечивая детекцию с учетом движения объектов. Использование нескольких ConvLSTM слоев поможет модели лучше различать случайные шумы и реальные перемещения зайцев.

### Оптимальные слои и их применение

- Сверточные слои (Convolutional Layers):
    Используются для экстракции признаков с разных уровней сложности. Начальные слои будут отвечать за общие текстуры и формы, а глубокие слои за более специфические признаки, отличающие зайцев от фона.

- Residual-блоки (Residual Blocks):
    Улучшают качество обучения в глубоких сетях за счет прямой передачи данных между слоями, минимизируя исчезновение градиента. В YOLO и EfficientDet они повышают точность детекции, особенно для мелких объектов.

- BiFPN (Bidirectional Feature Pyramid Network):
    Уникальный слой, который используется в EfficientDet для объединения признаков с разных уровней пирамиды. Подходит для улучшения распознавания объектов на фоне сложных условий, так как позволяет учитывать детали на разных масштабах.

- ROIAlign (Region of Interest Alignment):
    Используется в Mask R-CNN для точного выделения регионов интереса (ROI). Слой выравнивает признаки для более точной сегментации и хорошо подходит для распознавания объектов со сложной структурой или на неструктурированном фоне.

- Kalman Filter / Hungarian Algorithm для постпроцессинга:
    Эти алгоритмы помогают улучшить отслеживание объектов между кадрами, сглаживая траектории зайцев и уменьшая количество пропущенных объектов.

---

## Вопрос 3: Подготовка данных
### Организация сбора и разметки данных:
1. **Сбор данных**: запись видео с камеры, дополнение данными из открытых источников (например, [iNaturalist](https://github.com/visipedia/inat_comp) или Snapshot Serengeti).
2. **Разметка**: ручная разметка или полуавтоматическая разметка с предобученными моделями (LabelImg, CVAT).
3. **Аугментация данных**: изменение яркости, контраста, добавление шума, что повышает устойчивость к условиям среды.

### Решение сложностей:
- **Сложное освещение и фоновый шум**: аугментация и использование сегментации (Mask R-CNN).
- **Частичное перекрытие объектов**: использование моделей, способных учитывать мелкие детали и сложный фон.

---

## Вопрос 4: Оптимизация и снижение погрешности
### Методы минимизации ложных срабатываний (FP) и пропусков (FN):
1. **Многоуровневая детекция и сегментация**: Mask R-CNN для точных границ, FPN для объектов разного масштаба.
    - Использование архитектуры, сочетающей детекцию и сегментацию, например Mask R-CNN, поможет точнее определять границы объектов и выделять их от фона. Сегментация позволяет снизить FP, так как модель будет учитывать не только форму, но и контуры объектов, что снижает вероятность ложных срабатываний на объектах, напоминающих зайцев.
    - Для дальнейшего повышения точности можно комбинировать YOLO для первичной детекции и Mask R-CNN для уточнения границ.
    - Включение Feature Pyramid Network (FPN), как в архитектуре EfficientDet, помогает учитывать как крупные, так и мелкие детали, что важно для детекции зайцев на различной удаленности. Это снижает как FP, так и FN за счет обработки объектов разного масштаба и улучшения адаптации модели к сложному фону.
2. **Аугментация и временные признаки**: улучшает распознавание в условиях сложного фона (ConvLSTM, 3D-CNN).
    - Разнообразные методы аугментации, такие как изменение яркости, контраста, добавление шума, повороты и увеличение, помогут модели распознавать зайцев в изменяющихся условиях. Это снизит FN за счет лучшего распознавания объектов, например, в тени или в условиях плотной растительности.
    - Использование 3D-CNN или ConvLSTM слоев добавит временную компоненту, позволяя учитывать движения объектов между кадрами. Это помогает избежать FP, так как модель научится различать случайные движения фона (ветви, трава) от движений зайцев.
3. **Постобработка**: фильтрация детекции по вероятности, контекстная фильтрация для устранения шума (фильтр Калмана).
    - Включение контекстной фильтрации: например, если заяц не может резко менять положение в кадре, то резкое появление «нового объекта» без движения может быть помечено как FP.
    - Применение фильтра Калмана или алгоритма Хангариана для отслеживания зайцев поможет снизить количество ложных срабатываний на объектах, которые «могут показаться зайцами» из-за их формы или текстуры.
    - Если объект виден на нескольких кадрах подряд, это подтверждает его наличие, и, наоборот, появление объекта только в одном кадре может быть отфильтровано как FP.
    - Использование метрики Intersection over Union (IoU) на последовательных кадрах для проверки соответствия между предполагаемыми позициями зайца поможет снизить ложные срабатывания.
4. **Дообучение и активное обучение**: добавление сложных примеров для повышения устойчивости модели.
    - Использование активного обучения для регулярного добавления в тренировочный набор ошибочных детекций (ложные срабатывания и пропуски) позволит модели учиться на своих ошибках и адаптироваться к новым условиям фона. Это повысит её устойчивость и снизит FN.
5. **Калибровка вероятностного порога**
    - Установка порога уверенности модели так, чтобы исключить объекты с низкой вероятностью детекции. Это позволит снизить FP. Выбор порога на основе анализа ROC-кривой и Precision-Recall кривой может помочь выбрать оптимальное значение.
6. **Фильтрация изображений от фоновых шумов**
    - Применение цветовых фильтров или предобработки на основе контрастности может выделить объекты, схожие с зайцами, от других шумов, таких как листья, ветки и трава.
    - Использование фильтров на основе порогового значения для удаления очень мелких объектов, которые не могут быть зайцами, также снизит количество FP.

---

## Вопрос 5: Производительность
Чтобы обеспечить высокую производительность и скорость работы модели в реальном времени при детекции объектов, необходимо оптимизировать как модель, так и процесс обработки данных.  
### Методы оптимизации:
1. **Легкие архитектуры (YOLOv5-Tiny, EfficientDet)**: подходит для работы на ограниченных ресурсах, в том числе CPU.
2. **Техники оптимизации модели**: квантование, прунинг, distillation (TensorFlow Lite, ONNX).
    - Post-training quantization: преобразование весов модели с 32-битных до 8-битных целых чисел (int8), что позволяет значительно снизить объем памяти и увеличить скорость выполнения. Современные библиотеки, такие как TensorFlow Lite или PyTorch Mobile, поддерживают квантование без значительной потери точности.
    - Quantization-aware training (QAT): при обучении модель учитывает квантованные веса, что минимизирует потерю точности после квантования. Это полезно, если вы ожидаете более жесткие ограничения по ресурсам.
    - Постепенное обрезание несущественных весов в модели, таких как нейроны или каналы, которые меньше всего влияют на точность. Это делает модель легче и увеличивает производительность. Например, метод прунинга каналов можно использовать для уменьшения вычислительной нагрузки без значительной потери точности.
    - Прунинг может быть выполнен на основе slim-критерия, удаляя нейроны с минимальным вкладом, или метода Лотти (Lottery Ticket Hypothesis), где остаются только важные веса.
    -**Knowledge Distillation (KD)** - Перенос знаний из «большой» модели в «маленькую» с использованием процесса distillation, где меньшая модель (student) учится имитировать поведение более мощной модели (teacher). KD позволяет сократить размер модели и ускорить ее работу, особенно если начальная модель слишком велика для работы в реальном времени.
3. **Оптимизация инференса**: использование TensorRT для GPU, микробатчинг.
    - TensorRT от NVIDIA — это оптимизатор инференса, который улучшает производительность на GPU за счет автоматического преобразования модели, применения квантования и оптимизации графа вычислений. Подходит для моделей, которые будут запускаться на устройствах с GPU, таких как NVIDIA Jetson.
    - TensorFlow Lite (TFLite) и ONNX (Open Neural Network Exchange) — это фреймворки для мобильных устройств и серверов, которые поддерживают оптимизацию моделей. TFLite поддерживает квантование и оптимизацию для ARM-процессоров, тогда как ONNX может быть оптимизирован для ускоренного инференса с ONNX Runtime на CPU и GPU.
    - Подход с микро-батчами позволяет сгруппировать несколько кадров и провести детекцию «пакетно», что минимизирует задержки на каждый кадр. Буферизация позволяет предсказывать положение зайцев на ближайшие кадры, сокращая задержку при инференсе.
4. **Ресурсоэффективная обработка видео**: снижение разрешения, использование ROI для фокусировки на интересующих областях.
    - Обработка видео с более низким разрешением (например, уменьшение до 720p или даже 480p), если это не критично для точности. Это уменьшит объем вычислений, особенно на этапах предобработки.
    - Использование ROI (Region of Interest) для сокращения объема анализа, ограничиваясь только зонами, где возможна активность зайцев. Это особенно полезно, если камера закреплена и видимая область не меняется.
    - Обработка не каждого кадра, а выборка через определенные промежутки времени, например, каждый второй или третий кадр, позволяет экономить ресурсы. Время обработки сокращается, но зайцы продолжают отслеживаться с минимальной потерей точности, особенно если скорость их движения невелика.

### Аппаратные решения:
- **NVIDIA Jetson, Google Coral**: подходят для работы в полевых условиях, поддерживают оптимизации, такие как квантование и ускоренный инференс.

### Распараллеливание обработки данных:
- Использование Dask или Ray для распределенной обработки данных, если данные поступают с нескольких камер. Это позволяет масштабировать систему и обрабатывать несколько видеопотоков параллельно.
- Разделение этапов обработки между CPU и GPU: предобработка видео (такие как уменьшение разрешения и фильтрация) может выполняться на CPU, в то время как инференс по модели производится на GPU
---

## Вопрос 6: Постобработка и интеграция
### Постобработка для повышения качества:
1. **Отслеживание объектов и фильтрация траекторий**: фильтр Калмана, алгоритм Хангариана для точной идентификации.  
    - Алгоритм фильтра Калмана: Использование фильтра Калмана помогает сгладить траектории объектов, минимизируя внезапные скачки и улучшая устойчивость к ложным срабатываниям (FP), особенно при кратковременных потерях объекта в кадре. Алгоритм предсказывает следующую позицию объекта, основываясь на его предыдущей скорости и направлении, что позволяет фильтровать случайные шумы.
    - Алгоритм Хангариана: Этот алгоритм для сопоставления между кадрами поможет удерживать идентичность объекта, если несколько зайцев находятся рядом друг с другом, улучшая точность и предотвращая потерю объектов (FN).
    - Включение вероятностного порога для фильтрации детекций с низким уровнем уверенности модели. Например, если вероятность ниже определенного порога (например, 0.5), то детекция может быть отброшена, так как она может быть ложным срабатыванием.

2. **Контекстуальные фильтры и маски**: исключение зон без объектов интереса, например, небо или дорога.  
    - Маскирование зон, не представляющих интерес: Если в кадре есть участки, где зайцы появляться не могут (например, дорога или небо), можно использовать статические маски для исключения этих зон из анализа.
    - Контекстуальные правила: Например, если заяц не может резко переместиться на значительное расстояние за один кадр, можно фильтровать подобные детекции, что снижает количество ложных срабатываний при сложном фоне.

3. **Усреднение вероятностей детекции**: для каждого объекта можно использовать усреднение вероятностей на нескольких последовательных кадрах для более точного предсказания. Это снижает влияние случайных ложных срабатываний и делает систему более устойчивой к кратковременным искажениям, таким как изменения освещения.

4. **Использование IoU (Intersection over Union) для контроля последовательности детекций**: вычисление IoU между предсказанными боксами объекта в текущем и предыдущих кадрах помогает идентифицировать последовательные кадры одного и того же объекта. Низкий IoU может означать случайное ложное срабатывание, и такие детекции могут быть отброшены.


### Интеграция в видеопоток:
1. **Потоковая обработка**: использование OpenCV или GStreamer, промежуточного хранилища.
    - Использование OpenCV или GStreamer для захвата и предварительной обработки видеопотока в реальном времени. Это позволит обрабатывать кадры непосредственно с камеры и передавать их в модель для инференса.
    - Использование промежуточного хранилища (например, ZeroMQ или Redis) для передачи данных между компонентами системы , что обеспечит гибкость и устойчивость к сбоям в системе.
2. **Разделение на потоки и буферизация**:
    - Использование многопоточной обработки для разделения обработки на несколько этапов: захват видео, инференс, постобработка и визуализация. Например, один поток может выполнять захват видео, другой — инференс, третий — постобработку и запись/визуализацию результатов.
    - Добавление буферизации (например, кольцевого буфера), чтобы обрабатывать кадры асинхронно, это поможет предотвратить задержки при высоком разрешении видео или при ограниченных ресурсах.
3. **Вывод результатов и уведомления**: сохранение результатов в JSON/CSV, интеграция с аналитикой, настройка уведомлений.
    - Визуализация результата на реальном видеопотоке: использование OpenCV для наложения боксов и траекторий зайцев на исходный видеопоток, с добавлением индикаторов вероятностей, а также идентификаторов для каждого зайца, чтобы отслеживать их по кадрам.
    - Сохранение результатов в формате JSON или CSV (с координатами объектов и вероятностями детекции) для дальнейшего анализа. Результаты можно хранить в базе данных или передавать в систему аналитики в режиме реального времени.
4. **Масштабируемость**: динамическая регулировка частоты обработки, использование Docker для гибкого развертывания.
    - В зависимости от загруженности системы, можно адаптивно менять частоту обработки кадров (например, обрабатывать каждый второй кадр при высокой нагрузке) или разрешение кадров, чтобы снизить нагрузку и избежать потери производительности.



